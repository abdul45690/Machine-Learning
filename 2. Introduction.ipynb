{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc0ea1f5",
   "metadata": {},
   "source": [
    "# Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "|Overfitting|Underfitting|\n",
    "|---|---|\n",
    "|When a model is too complex and fits the training data too closely, including the noise in the data.|When a model is too simple to capture the underlying patterns in the data, resulting in poor performance on both the training and test data. |\n",
    "|Result: High Train accuracy so low bias|Low Train accuracy so high bias|\n",
    "|Result: Low Test accuracy so high variance|Low Test accuracy so high variance|\n",
    "\n",
    "## `Mitigate Overfitting:`\n",
    "\n",
    "- L1 regularization: This technique adds a penalty term that is proportional to the `absolute value of the weights` in the model. L1 regularization encourages the model to `select a subset of the most important features and set the weights of the other features to zero`. This can be useful for feature selection and reducing the complexity of the model.\n",
    "\n",
    "- L2 regularization: This technique adds a penalty term that is proportional to the `square of the weights` in the model. L2 regularization encourages the model to `spread the weight values across all the features rather than relying on a few dominant features`. This can improve the generalization performance of the model.\n",
    "- Regularization: By adding a penalty term to the objective function during training, we can encourage the model to prefer simpler solutions and avoid overfitting.\n",
    "- Dropout: By randomly dropping out some of the neurons in a neural network during training, we can prevent the network from relying too heavily on any particular feature or combination of features.\n",
    "- Early stopping: By monitoring the performance of the model on a validation set during training, we can stop the training early when the validation performance starts to deteriorate, indicating that the model is starting to overfit.\n",
    "- Cross-validation: Cross-validation is a technique used to assess the performance of a model on new, unseen data. This is done by dividing the data into training and validation sets and then repeating this process multiple times with different splits. By doing this, we can get a better estimate of the model's performance on new data and prevent overfitting.\n",
    "\n",
    "- Data augmentation: Data augmentation is a technique used to artificially increase the size of the training set by creating new, slightly modified versions of the existing data. By doing this, we can prevent overfitting by exposing the model to more variations of the data.\n",
    "\n",
    "## `Mitigate Underfitting:`\n",
    "- Feature engineering: By transforming or combining the input features, we can create new features that capture the underlying patterns in the data more effectively.\n",
    "- Model complexity: By increasing the complexity of the model, we can allow it to capture more complex patterns in the data.\n",
    "- Ensemble methods: By combining multiple weak models into a stronger ensemble, we can improve the accuracy and reduce the risk of underfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106ce0f8",
   "metadata": {},
   "source": [
    "# Q2: How can we reduce overfitting? Explain in brief.\n",
    "Refer Q.1. 'Mitigate Overfitting:' part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b81fc6",
   "metadata": {},
   "source": [
    "# Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Refer Q.1. 'Underfitting' part\n",
    "\n",
    "\n",
    "The underfitting can occur due to:\n",
    "- Insufficient data\n",
    "- Insufficient features\n",
    "\n",
    "- Over-regularization\n",
    "- High bias model\n",
    "\n",
    "- Poor model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c906269",
   "metadata": {},
   "source": [
    "# Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "Bias-variance tradeoff describes the relationship between model complexity, model performance, and the ability of the model to generalize to new, unseen data.\n",
    "\n",
    "Bias refers to the difference between the expected prediction of the model and the true value of the target variable\n",
    "\n",
    "Variance refers to the variability of the model's predictions for different training sets.\n",
    "\n",
    "As we get low bias and low variance we get high performance model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cacc31",
   "metadata": {},
   "source": [
    "# Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "- `Visual inspection`: Plotting the `learning curves` for both the training and validation datasets. If the training error is significantly lower than the validation error, it indicates overfitting, while if both errors are high, it indicates underfitting.\n",
    "\n",
    "- `Cross-validation`:  If the model performs well on the training data but poorly on the validation data, it indicates overfitting, while poor performance on both training and validation data indicates underfitting.\n",
    "\n",
    "- `Regularization`:  If the regularization strength is too high, it can lead to underfitting.\n",
    "\n",
    "- `Feature importance`: If the model is overfitting, it may be giving too much importance to certain features that are not important for the task, resulting in poor generalization. Feature importance analysis can help identify such features.\n",
    "\n",
    "- `Ensemble methods`: Ensemble methods such as bagging, boosting, and stacking can help reduce overfitting by combining multiple models and reducing the variance of the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e6e3b5",
   "metadata": {},
   "source": [
    "\n",
    "# Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "Refer Q.1.  \n",
    "\n",
    "high bias and high variance models is our underfitting model  for example: \n",
    "\n",
    "linear regression models that assume a linear relationship between the input features and the target variable, while the true relationship is non-linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000f8cd",
   "metadata": {},
   "source": [
    "# Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "Refer Q.1 and Q.5\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
